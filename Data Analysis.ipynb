{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Data Analysis.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY1diH_0s_cD"
      },
      "source": [
        "# Fraud Detection Explorotary Data Analysis of the Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob8MhVmms_cE",
        "outputId": "fadefd42-ca7f-48e9-e9ac-855df76b7a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Import of all the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.stats.api as sms\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import pickle"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-Ry7EiTs_cK"
      },
      "source": [
        "## Import of Dataset\n",
        "\n",
        "Import of the datasets into Panda dataframe\n",
        "\n",
        "Initial modifications to add an ID and use it instead of the Time \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2RxMqHnBqyX",
        "outputId": "ae410327-96c4-40bb-f023-863a62d1c4ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Connecting the Google drive with Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGY0LTIVs_cL",
        "outputId": "a450df30-6eca-4d65-daef-053e47ef2ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "# Import of the train and test datasets into the Panda dataframe from Google drive \n",
        "# To load data from the local drive the location below can be changed accordingly\n",
        "\n",
        "cc_test_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/creditcard_test.csv\")\n",
        "cc_train_df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/creditcard_train.csv\")\n",
        "\n",
        "cc_test_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>71852.0</td>\n",
              "      <td>-0.584007</td>\n",
              "      <td>0.880079</td>\n",
              "      <td>1.320090</td>\n",
              "      <td>0.001509</td>\n",
              "      <td>0.292260</td>\n",
              "      <td>-0.990634</td>\n",
              "      <td>1.120755</td>\n",
              "      <td>-0.166826</td>\n",
              "      <td>-0.501695</td>\n",
              "      <td>-0.694423</td>\n",
              "      <td>-0.706512</td>\n",
              "      <td>-0.164179</td>\n",
              "      <td>-0.276469</td>\n",
              "      <td>0.284019</td>\n",
              "      <td>0.164088</td>\n",
              "      <td>0.265888</td>\n",
              "      <td>-0.739878</td>\n",
              "      <td>-0.034332</td>\n",
              "      <td>-1.077470</td>\n",
              "      <td>-0.106804</td>\n",
              "      <td>0.167209</td>\n",
              "      <td>0.361133</td>\n",
              "      <td>-0.125141</td>\n",
              "      <td>0.375231</td>\n",
              "      <td>0.029598</td>\n",
              "      <td>-0.559438</td>\n",
              "      <td>0.102098</td>\n",
              "      <td>0.144502</td>\n",
              "      <td>42.81</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>48336.0</td>\n",
              "      <td>1.326714</td>\n",
              "      <td>-0.823441</td>\n",
              "      <td>-0.127404</td>\n",
              "      <td>-0.979830</td>\n",
              "      <td>-0.625116</td>\n",
              "      <td>-0.104941</td>\n",
              "      <td>-0.636415</td>\n",
              "      <td>0.050208</td>\n",
              "      <td>-0.785860</td>\n",
              "      <td>0.815473</td>\n",
              "      <td>0.200606</td>\n",
              "      <td>-1.002079</td>\n",
              "      <td>-1.025594</td>\n",
              "      <td>0.271240</td>\n",
              "      <td>0.320012</td>\n",
              "      <td>1.751346</td>\n",
              "      <td>-0.362698</td>\n",
              "      <td>-0.350818</td>\n",
              "      <td>1.235828</td>\n",
              "      <td>0.164916</td>\n",
              "      <td>0.109578</td>\n",
              "      <td>-0.011447</td>\n",
              "      <td>-0.212676</td>\n",
              "      <td>-0.859332</td>\n",
              "      <td>0.507870</td>\n",
              "      <td>-0.199817</td>\n",
              "      <td>-0.021529</td>\n",
              "      <td>0.003230</td>\n",
              "      <td>75.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>166298.0</td>\n",
              "      <td>2.097691</td>\n",
              "      <td>-0.076097</td>\n",
              "      <td>-1.743719</td>\n",
              "      <td>-0.011746</td>\n",
              "      <td>0.597501</td>\n",
              "      <td>-0.332470</td>\n",
              "      <td>0.130716</td>\n",
              "      <td>-0.174237</td>\n",
              "      <td>0.383371</td>\n",
              "      <td>0.171848</td>\n",
              "      <td>0.110069</td>\n",
              "      <td>0.740958</td>\n",
              "      <td>0.344720</td>\n",
              "      <td>0.589561</td>\n",
              "      <td>0.147784</td>\n",
              "      <td>0.230686</td>\n",
              "      <td>-1.103407</td>\n",
              "      <td>0.675318</td>\n",
              "      <td>0.395078</td>\n",
              "      <td>-0.192933</td>\n",
              "      <td>0.233496</td>\n",
              "      <td>0.825427</td>\n",
              "      <td>-0.143247</td>\n",
              "      <td>-1.115397</td>\n",
              "      <td>0.374189</td>\n",
              "      <td>-0.056085</td>\n",
              "      <td>-0.020643</td>\n",
              "      <td>-0.079921</td>\n",
              "      <td>2.29</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40650.0</td>\n",
              "      <td>-1.593912</td>\n",
              "      <td>2.215525</td>\n",
              "      <td>0.210067</td>\n",
              "      <td>-0.236255</td>\n",
              "      <td>-0.002224</td>\n",
              "      <td>-0.058601</td>\n",
              "      <td>-0.392776</td>\n",
              "      <td>-2.005520</td>\n",
              "      <td>-0.306309</td>\n",
              "      <td>-0.253465</td>\n",
              "      <td>-0.892002</td>\n",
              "      <td>0.681270</td>\n",
              "      <td>1.539975</td>\n",
              "      <td>-0.407657</td>\n",
              "      <td>0.740105</td>\n",
              "      <td>0.687552</td>\n",
              "      <td>-0.019201</td>\n",
              "      <td>0.040053</td>\n",
              "      <td>0.133313</td>\n",
              "      <td>-0.258684</td>\n",
              "      <td>1.995801</td>\n",
              "      <td>-1.554302</td>\n",
              "      <td>0.261539</td>\n",
              "      <td>-0.531345</td>\n",
              "      <td>0.124584</td>\n",
              "      <td>0.146124</td>\n",
              "      <td>0.346683</td>\n",
              "      <td>0.104860</td>\n",
              "      <td>8.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>121428.0</td>\n",
              "      <td>1.863194</td>\n",
              "      <td>0.265005</td>\n",
              "      <td>-0.261564</td>\n",
              "      <td>3.853830</td>\n",
              "      <td>0.220877</td>\n",
              "      <td>0.773558</td>\n",
              "      <td>-0.296455</td>\n",
              "      <td>0.207090</td>\n",
              "      <td>-0.487512</td>\n",
              "      <td>1.454433</td>\n",
              "      <td>-0.245666</td>\n",
              "      <td>0.610215</td>\n",
              "      <td>-0.153871</td>\n",
              "      <td>-0.147691</td>\n",
              "      <td>-2.586887</td>\n",
              "      <td>0.606259</td>\n",
              "      <td>-0.701786</td>\n",
              "      <td>0.034701</td>\n",
              "      <td>-0.923304</td>\n",
              "      <td>-0.311545</td>\n",
              "      <td>0.118870</td>\n",
              "      <td>0.611417</td>\n",
              "      <td>0.034731</td>\n",
              "      <td>-0.387779</td>\n",
              "      <td>0.103574</td>\n",
              "      <td>0.140670</td>\n",
              "      <td>-0.003506</td>\n",
              "      <td>-0.061238</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85438</th>\n",
              "      <td>116793.0</td>\n",
              "      <td>-0.753070</td>\n",
              "      <td>1.600625</td>\n",
              "      <td>-1.694466</td>\n",
              "      <td>-1.448546</td>\n",
              "      <td>1.130943</td>\n",
              "      <td>-0.976420</td>\n",
              "      <td>1.167232</td>\n",
              "      <td>0.201294</td>\n",
              "      <td>-0.245013</td>\n",
              "      <td>0.113600</td>\n",
              "      <td>-0.376961</td>\n",
              "      <td>-0.064316</td>\n",
              "      <td>-1.027914</td>\n",
              "      <td>1.212622</td>\n",
              "      <td>-0.719329</td>\n",
              "      <td>-0.053734</td>\n",
              "      <td>-0.817776</td>\n",
              "      <td>0.558632</td>\n",
              "      <td>0.478907</td>\n",
              "      <td>0.050720</td>\n",
              "      <td>0.202703</td>\n",
              "      <td>0.725051</td>\n",
              "      <td>-0.303092</td>\n",
              "      <td>-1.016408</td>\n",
              "      <td>-0.107503</td>\n",
              "      <td>0.146474</td>\n",
              "      <td>0.469657</td>\n",
              "      <td>0.304103</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85439</th>\n",
              "      <td>57942.0</td>\n",
              "      <td>-0.539994</td>\n",
              "      <td>0.515511</td>\n",
              "      <td>2.029197</td>\n",
              "      <td>0.514863</td>\n",
              "      <td>-0.043655</td>\n",
              "      <td>-0.275000</td>\n",
              "      <td>0.851781</td>\n",
              "      <td>-0.059080</td>\n",
              "      <td>0.319422</td>\n",
              "      <td>-0.573884</td>\n",
              "      <td>-0.550373</td>\n",
              "      <td>-0.258060</td>\n",
              "      <td>-1.541622</td>\n",
              "      <td>-0.192496</td>\n",
              "      <td>-0.756751</td>\n",
              "      <td>-0.702496</td>\n",
              "      <td>0.159027</td>\n",
              "      <td>-0.787307</td>\n",
              "      <td>-1.142297</td>\n",
              "      <td>-0.173193</td>\n",
              "      <td>0.059047</td>\n",
              "      <td>0.441880</td>\n",
              "      <td>-0.048056</td>\n",
              "      <td>0.574042</td>\n",
              "      <td>-0.252453</td>\n",
              "      <td>-0.545212</td>\n",
              "      <td>-0.007875</td>\n",
              "      <td>-0.067811</td>\n",
              "      <td>40.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85440</th>\n",
              "      <td>53165.0</td>\n",
              "      <td>-1.833594</td>\n",
              "      <td>-1.015652</td>\n",
              "      <td>1.217378</td>\n",
              "      <td>-1.464593</td>\n",
              "      <td>-0.448623</td>\n",
              "      <td>-0.955303</td>\n",
              "      <td>-0.496368</td>\n",
              "      <td>0.481275</td>\n",
              "      <td>-1.151529</td>\n",
              "      <td>-0.210526</td>\n",
              "      <td>-1.014940</td>\n",
              "      <td>-0.517389</td>\n",
              "      <td>0.895314</td>\n",
              "      <td>-0.274270</td>\n",
              "      <td>0.320422</td>\n",
              "      <td>1.832300</td>\n",
              "      <td>-0.170460</td>\n",
              "      <td>-0.889855</td>\n",
              "      <td>-0.041644</td>\n",
              "      <td>0.754963</td>\n",
              "      <td>0.363656</td>\n",
              "      <td>0.279139</td>\n",
              "      <td>0.166895</td>\n",
              "      <td>-0.118669</td>\n",
              "      <td>-0.005576</td>\n",
              "      <td>-0.438610</td>\n",
              "      <td>0.209083</td>\n",
              "      <td>-0.049870</td>\n",
              "      <td>140.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85441</th>\n",
              "      <td>166438.0</td>\n",
              "      <td>1.373911</td>\n",
              "      <td>-0.999031</td>\n",
              "      <td>-1.694373</td>\n",
              "      <td>1.057884</td>\n",
              "      <td>-0.413288</td>\n",
              "      <td>-1.033805</td>\n",
              "      <td>0.412863</td>\n",
              "      <td>-0.382925</td>\n",
              "      <td>0.640802</td>\n",
              "      <td>-0.550427</td>\n",
              "      <td>-0.477887</td>\n",
              "      <td>0.329516</td>\n",
              "      <td>0.595095</td>\n",
              "      <td>-1.184722</td>\n",
              "      <td>0.311344</td>\n",
              "      <td>0.375467</td>\n",
              "      <td>0.528451</td>\n",
              "      <td>0.493846</td>\n",
              "      <td>-0.607897</td>\n",
              "      <td>0.537641</td>\n",
              "      <td>0.470784</td>\n",
              "      <td>0.868639</td>\n",
              "      <td>-0.348918</td>\n",
              "      <td>0.059266</td>\n",
              "      <td>0.041987</td>\n",
              "      <td>0.580518</td>\n",
              "      <td>-0.086933</td>\n",
              "      <td>0.025684</td>\n",
              "      <td>343.39</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85442</th>\n",
              "      <td>1480.0</td>\n",
              "      <td>-1.683357</td>\n",
              "      <td>1.505123</td>\n",
              "      <td>0.843921</td>\n",
              "      <td>-0.827478</td>\n",
              "      <td>-0.431862</td>\n",
              "      <td>-0.215336</td>\n",
              "      <td>-0.102517</td>\n",
              "      <td>-0.114782</td>\n",
              "      <td>0.349091</td>\n",
              "      <td>-0.069912</td>\n",
              "      <td>0.211948</td>\n",
              "      <td>0.290004</td>\n",
              "      <td>-1.516944</td>\n",
              "      <td>0.405564</td>\n",
              "      <td>-1.273634</td>\n",
              "      <td>0.224752</td>\n",
              "      <td>-0.202278</td>\n",
              "      <td>0.129105</td>\n",
              "      <td>0.255646</td>\n",
              "      <td>-0.063979</td>\n",
              "      <td>0.558429</td>\n",
              "      <td>-0.712699</td>\n",
              "      <td>-0.019037</td>\n",
              "      <td>-0.010307</td>\n",
              "      <td>0.128748</td>\n",
              "      <td>0.401953</td>\n",
              "      <td>0.392933</td>\n",
              "      <td>0.213428</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>85443 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Time        V1        V2  ...       V28  Amount  Class\n",
              "0       71852.0 -0.584007  0.880079  ...  0.144502   42.81      0\n",
              "1       48336.0  1.326714 -0.823441  ...  0.003230   75.00      0\n",
              "2      166298.0  2.097691 -0.076097  ... -0.079921    2.29      0\n",
              "3       40650.0 -1.593912  2.215525  ...  0.104860    8.99      0\n",
              "4      121428.0  1.863194  0.265005  ... -0.061238    0.00      0\n",
              "...         ...       ...       ...  ...       ...     ...    ...\n",
              "85438  116793.0 -0.753070  1.600625  ...  0.304103    0.75      0\n",
              "85439   57942.0 -0.539994  0.515511  ... -0.067811   40.00      0\n",
              "85440   53165.0 -1.833594 -1.015652  ... -0.049870  140.00      0\n",
              "85441  166438.0  1.373911 -0.999031  ...  0.025684  343.39      0\n",
              "85442    1480.0 -1.683357  1.505123  ...  0.213428    5.00      0\n",
              "\n",
              "[85443 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "shMKcBLKs_cS"
      },
      "source": [
        "# Functiona to create a list to be used as ID for the data frame \n",
        "def id_create(df):\n",
        "    id = []\n",
        "    for i in range(df.shape[0]):\n",
        "        id.append(i+1)\n",
        "    return id\n",
        "\n",
        "# For the test data frame created an ID list and added to the data frame \n",
        "id_test = id_create(cc_test_df)\n",
        "cc_test_df[\"Id\"] = id_test\n",
        "cc_test_df.set_index(\"Id\", inplace=True)\n",
        "\n",
        "# For the train data frame created an ID list and added to the data frame \n",
        "id_train = id_create(cc_train_df)\n",
        "cc_train_df[\"Id\"] = id_train\n",
        "cc_train_df.set_index(\"Id\", inplace=True)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFuZ-LW7s_cW"
      },
      "source": [
        "## Check of the missing value\n",
        "\n",
        "The dataframe information and description along with the missing label plots shows that there isn't any missing or NA value\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebnBXc4Ps_cW",
        "outputId": "976cbbfa-238e-4721-faca-d3df969f87be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"Dataframe information of the train dataset\")\n",
        "print(cc_train_df.info())\n",
        "print()\n",
        "print(\"Dataframe information of the test dataset\")\n",
        "print(cc_test_df.info())\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataframe information of the train dataset\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 199364 entries, 1 to 199364\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    199364 non-null  float64\n",
            " 1   V1      199364 non-null  float64\n",
            " 2   V2      199364 non-null  float64\n",
            " 3   V3      199364 non-null  float64\n",
            " 4   V4      199364 non-null  float64\n",
            " 5   V5      199364 non-null  float64\n",
            " 6   V6      199364 non-null  float64\n",
            " 7   V7      199364 non-null  float64\n",
            " 8   V8      199364 non-null  float64\n",
            " 9   V9      199364 non-null  float64\n",
            " 10  V10     199364 non-null  float64\n",
            " 11  V11     199364 non-null  float64\n",
            " 12  V12     199364 non-null  float64\n",
            " 13  V13     199364 non-null  float64\n",
            " 14  V14     199364 non-null  float64\n",
            " 15  V15     199364 non-null  float64\n",
            " 16  V16     199364 non-null  float64\n",
            " 17  V17     199364 non-null  float64\n",
            " 18  V18     199364 non-null  float64\n",
            " 19  V19     199364 non-null  float64\n",
            " 20  V20     199364 non-null  float64\n",
            " 21  V21     199364 non-null  float64\n",
            " 22  V22     199364 non-null  float64\n",
            " 23  V23     199364 non-null  float64\n",
            " 24  V24     199364 non-null  float64\n",
            " 25  V25     199364 non-null  float64\n",
            " 26  V26     199364 non-null  float64\n",
            " 27  V27     199364 non-null  float64\n",
            " 28  V28     199364 non-null  float64\n",
            " 29  Amount  199364 non-null  float64\n",
            " 30  Class   199364 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 48.7 MB\n",
            "None\n",
            "\n",
            "Dataframe information of the test dataset\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 85443 entries, 1 to 85443\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Time    85443 non-null  float64\n",
            " 1   V1      85443 non-null  float64\n",
            " 2   V2      85443 non-null  float64\n",
            " 3   V3      85443 non-null  float64\n",
            " 4   V4      85443 non-null  float64\n",
            " 5   V5      85443 non-null  float64\n",
            " 6   V6      85443 non-null  float64\n",
            " 7   V7      85443 non-null  float64\n",
            " 8   V8      85443 non-null  float64\n",
            " 9   V9      85443 non-null  float64\n",
            " 10  V10     85443 non-null  float64\n",
            " 11  V11     85443 non-null  float64\n",
            " 12  V12     85443 non-null  float64\n",
            " 13  V13     85443 non-null  float64\n",
            " 14  V14     85443 non-null  float64\n",
            " 15  V15     85443 non-null  float64\n",
            " 16  V16     85443 non-null  float64\n",
            " 17  V17     85443 non-null  float64\n",
            " 18  V18     85443 non-null  float64\n",
            " 19  V19     85443 non-null  float64\n",
            " 20  V20     85443 non-null  float64\n",
            " 21  V21     85443 non-null  float64\n",
            " 22  V22     85443 non-null  float64\n",
            " 23  V23     85443 non-null  float64\n",
            " 24  V24     85443 non-null  float64\n",
            " 25  V25     85443 non-null  float64\n",
            " 26  V26     85443 non-null  float64\n",
            " 27  V27     85443 non-null  float64\n",
            " 28  V28     85443 non-null  float64\n",
            " 29  Amount  85443 non-null  float64\n",
            " 30  Class   85443 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 20.9 MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KufWkGlPs_ca"
      },
      "source": [
        "# Missing label plot for the train dataset\n",
        "msno.matrix(cc_train_df, labels=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2rMeK0Ms_cd"
      },
      "source": [
        "# Missing label plot for the test dataset\n",
        "msno.matrix(cc_test_df, labels=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvtBl8Fos_ck",
        "outputId": "c0d6a2a3-8783-4ffc-bcb5-f05ee62a99be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "print(\"Dataframe description of the train dataset\")\n",
        "print(cc_train_df.describe())\n",
        "print()\n",
        "print(\"Dataframe description of the test dataset\")\n",
        "print(cc_test_df.describe())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataframe description of the train dataset\n",
            "                Time             V1  ...         Amount          Class\n",
            "count  199364.000000  199364.000000  ...  199364.000000  199364.000000\n",
            "mean    94806.201064       0.004189  ...      88.761549       0.001826\n",
            "std     47511.693849       1.954728  ...     253.776731       0.042691\n",
            "min         0.000000     -56.407510  ...       0.000000       0.000000\n",
            "25%     54206.250000      -0.918115  ...       5.500000       0.000000\n",
            "50%     84650.000000       0.023565  ...      21.990000       0.000000\n",
            "75%    139350.250000       1.317063  ...      77.000000       0.000000\n",
            "max    172792.000000       2.454930  ...   25691.160000       1.000000\n",
            "\n",
            "[8 rows x 31 columns]\n",
            "\n",
            "Dataframe description of the test dataset\n",
            "                Time            V1  ...        Amount         Class\n",
            "count   85443.000000  85443.000000  ...  85443.000000  85443.000000\n",
            "mean    94831.729164     -0.009775  ...     87.388463      0.001498\n",
            "std     47433.429276      1.967899  ...    241.371519      0.038676\n",
            "min         4.000000    -41.928738  ...      0.000000      0.000000\n",
            "25%     54200.000000     -0.925530  ...      5.800000      0.000000\n",
            "50%     84794.000000      0.004117  ...     22.120000      0.000000\n",
            "75%    139249.000000      1.311441  ...     77.900000      0.000000\n",
            "max    172784.000000      2.451888  ...  19656.530000      1.000000\n",
            "\n",
            "[8 rows x 31 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6nTgbi2s_cs"
      },
      "source": [
        "## Duplicate values\n",
        "\n",
        "Duplicate in the training dataset are dropped and the on in the test dataset are kept\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAWlxkDzs_ct",
        "outputId": "2a5029ed-a43a-4ec4-bfed-86a7c6ecb77e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Dupplicate in the train dataset\n",
        "print(\"Number of dupplicate in the training set \", cc_train_df.duplicated().sum())\n",
        "\n",
        "# Dupplicate in the test dataset\n",
        "print(\"Number of dupplicate in the test set \", cc_test_df.duplicated().sum())\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of dupplicate in the training set  585\n",
            "Number of dupplicate in the test set  131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RtuClcSs_c0"
      },
      "source": [
        "# Dropping the duplicate and keeping the first of the training  dataset\n",
        "cc_train_df.drop_duplicates(keep='first', inplace=True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6TphZ8Ps_c5",
        "outputId": "137fbec8-8d34-4daa-d096-e77db7ee55ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Dupplicate in the train dataset\n",
        "print(\"Number of dupplicate in the training set \", cc_train_df.duplicated().sum())\n",
        "\n",
        "# Dupplicate in the test dataset\n",
        "print(\"Number of dupplicate in the test set \", cc_test_df.duplicated().sum())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of dupplicate in the training set  0\n",
            "Number of dupplicate in the test set  131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdfurlpvs_c8"
      },
      "source": [
        "## Data type\n",
        "\n",
        "The Class column in data frame is of type int64\n",
        "It's value can be only 0 o 1, keep as it is so the correlation can be correlated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJV8DvDQs_c8"
      },
      "source": [
        "# Conversion to a category type\n",
        "#cc_train_df[\"Class\"] = cc_train_df[\"Class\"].astype(\"category\")\n",
        "#cc_train_df[\"Class\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v3zjJnts_dA"
      },
      "source": [
        "# Conversion to a category type\n",
        "#cc_test_df[\"Class\"] = cc_test_df[\"Class\"].astype(\"category\")\n",
        "#cc_test_df[\"Class\"].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEBurZ78s_dF"
      },
      "source": [
        "## Datset Corelation\n",
        "\n",
        "Exploring the dataset correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E_vNrwLs_dG",
        "outputId": "ac1ecd39-d4a6-4793-db31-e921db0b575e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "source": [
        "# list of all the columns minus the \"Class\"\n",
        "columns = cc_test_df.columns.tolist()\n",
        "columns_to_remove = [\"Class\"]\n",
        "\n",
        "## Remove a list from another list\n",
        "columns = [var for var in columns if var not in columns_to_remove]\n",
        "columns"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time',\n",
              " 'V1',\n",
              " 'V2',\n",
              " 'V3',\n",
              " 'V4',\n",
              " 'V5',\n",
              " 'V6',\n",
              " 'V7',\n",
              " 'V8',\n",
              " 'V9',\n",
              " 'V10',\n",
              " 'V11',\n",
              " 'V12',\n",
              " 'V13',\n",
              " 'V14',\n",
              " 'V15',\n",
              " 'V16',\n",
              " 'V17',\n",
              " 'V18',\n",
              " 'V19',\n",
              " 'V20',\n",
              " 'V21',\n",
              " 'V22',\n",
              " 'V23',\n",
              " 'V24',\n",
              " 'V25',\n",
              " 'V26',\n",
              " 'V27',\n",
              " 'V28',\n",
              " 'Amount']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfEPrVH7vGyj"
      },
      "source": [
        "# Tried to run the below code to get the picture for all the features but it wouldn't show all the picture, tried VSCode and Jupyter notebook\n",
        "# So i will plot the each of the feature separately\n",
        "'''\n",
        "for i, cat in enumerate(columns):\n",
        "  fig = px.scatter(cc_train_df, x=cat, y=\"Class\")\n",
        "  fig.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fulr83n2s_dK"
      },
      "source": [
        "# Function to plot the scatter plot and violin plot\n",
        "\n",
        "def plotting(x, y):\n",
        "    title = \"Scatter plot training set \" + x + \" vs \" + y\n",
        "    fig = px.scatter(cc_train_df, x=x, y=y, title=title)\n",
        "    fig.show()\n",
        "\n",
        "    title = \"Violin plot training set \" + x + \" vs \" + y\n",
        "    fig = px.violin(cc_train_df, y=y, x=x, box=True, title=title)\n",
        "    fig.update_layout(xaxis_type=\"category\", xaxis={'categoryorder':'mean ascending'})\n",
        "    fig.show()\n",
        "\n",
        "    title = \"Test set \" + x + \" vs \" + y\n",
        "    fig = px.scatter(cc_test_df, x=x, y=y, title=title)\n",
        "    fig.show()\n",
        "\n",
        "    title = \"Violin plot test set \" + x + \" vs \" + y\n",
        "    fig = px.violin(cc_test_df, y=y, x=x, box=True, title=title)\n",
        "    fig.update_layout(xaxis_type=\"category\", xaxis={'categoryorder':'mean ascending'})\n",
        "    fig.show()\n",
        "  \n",
        "  "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3HDK859s_dP"
      },
      "source": [
        "# Correlation of the Time\n",
        "\n",
        "plotting(\"Class\", \"Time\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA8qLadZvmPU"
      },
      "source": [
        "There is correlation Between Time and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers none"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rQduW9jvoKr"
      },
      "source": [
        "# Correlation of the V1\n",
        "plotting(\"Class\", \"V1\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRRt2RQ83HW0"
      },
      "source": [
        "There is correlation Between V1 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers below -40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKg4BYM3XUn"
      },
      "source": [
        "# Correlation of the V2\n",
        "plotting(\"Class\", \"V2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjqzzZUy32O0"
      },
      "source": [
        "There is correlation Between V2 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers below -40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trm0XSvG4E_g"
      },
      "source": [
        "# Correlation of the V3\n",
        "plotting(\"Class\", \"V3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdV_iWxf4ad7"
      },
      "source": [
        "There is correlation Between V3 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers below -35"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p_6-2BYBM2o"
      },
      "source": [
        "# Correlation of the V4\n",
        "plotting(\"Class\", \"V4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4monqg6DBTSN"
      },
      "source": [
        "There is correlation Between V4 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK0sNyLzCPqC"
      },
      "source": [
        "# Correlation of the V5\n",
        "plotting(\"Class\", \"V5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnpbLEwQDjtW"
      },
      "source": [
        "There is correlation Between V5 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 40 and less than 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbs1dagTCQ7V"
      },
      "source": [
        "# Correlation of the V6\n",
        "plotting(\"Class\", \"V6\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZIuFq2MDr6R"
      },
      "source": [
        "There is correlation Between V6 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "\n",
        "Outliers greater than 25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqnMH_eECQel"
      },
      "source": [
        "# Correlation of the V7\n",
        "plotting(\"Class\", \"V7\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYSgplfYDvev"
      },
      "source": [
        "There is correlation Between V7 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 60"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkDxYJ_8CScN"
      },
      "source": [
        "# Correlation of the V8\n",
        "plotting(\"Class\", \"V8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HOwzHXYEYUr"
      },
      "source": [
        "There is correlation Between V4 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers none"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0umuddtsCTJj"
      },
      "source": [
        "# Correlation of the V9\n",
        "plotting(\"Class\", \"V9\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPpc4iYtE4m1"
      },
      "source": [
        "There is correlation Between V9 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHMmMSPDCR4m"
      },
      "source": [
        "# Correlation of the V10\n",
        "plotting(\"Class\", \"V10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZj3jYjLE8Wf"
      },
      "source": [
        "There is correlation Between V10 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqunYmW9CdMm"
      },
      "source": [
        "# Correlation of the V11\n",
        "plotting(\"Class\", \"V11\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EgyfngjFKd7"
      },
      "source": [
        "There is correlation Between V11 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers none"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNdkEv48CeoL"
      },
      "source": [
        "# Correlation of the V12\n",
        "plotting(\"Class\", \"V12\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP7Z0YMXFQTb"
      },
      "source": [
        "There is correlation Between V12 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers none"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCE4JXoyCffJ"
      },
      "source": [
        "# Correlation of the V13\n",
        "plotting(\"Class\", \"V13\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0xQsNBwFWOM"
      },
      "source": [
        "There isn't any correlation Between V13 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA9hsN4-CeLd"
      },
      "source": [
        "# Correlation of the V14\n",
        "plotting(\"Class\", \"V14\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOhatum8FgYV"
      },
      "source": [
        "There is correlation Between V14 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers none"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpOdbMcCgiy"
      },
      "source": [
        "# Correlation of the V15\n",
        "plotting(\"Class\", \"V15\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV32bSiSFk5s"
      },
      "source": [
        "There isn't correlation Between V15 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSoEjdiaCgHu"
      },
      "source": [
        "# Correlation of the V16\n",
        "plotting(\"Class\", \"V16\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnmc_gOnFrcp"
      },
      "source": [
        "There is correlation Between V16 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thd6Sy2GChQ-"
      },
      "source": [
        "# Correlation of the V17\n",
        "plotting(\"Class\", \"V17\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR_IylJfFvOm"
      },
      "source": [
        "There is correlation Between V17 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers none"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq5SO8EzCiiP"
      },
      "source": [
        "# Correlation of the V18\n",
        "plotting(\"Class\", \"V18\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfneyzIWFy5G"
      },
      "source": [
        "There is correlation Between V18 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers none"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoGWlXj0CiCn"
      },
      "source": [
        "# Correlation of the V19\n",
        "plotting(\"Class\", \"V19\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbpbXHVQF4cp"
      },
      "source": [
        "There is correlation Between V19 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers less than -6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFq-NU3jC7fa"
      },
      "source": [
        "# Correlation of the V20\n",
        "plotting(\"Class\", \"V20\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRX2WZ7UGBe2"
      },
      "source": [
        "There is correlation Between V20 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers less than -40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klWEer3-DCOR"
      },
      "source": [
        "# Correlation of the V21\n",
        "plotting(\"Class\", \"V21\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P_kwf6FGHbu"
      },
      "source": [
        "There is correlation Between V21 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 20 and less than -20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwit1D9QDDqR"
      },
      "source": [
        "# Correlation of the V22\n",
        "plotting(\"Class\", \"V22\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxYsHMa7GNJ_"
      },
      "source": [
        "There is correlation Between V22 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 10 and less than 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZu1c60eDEwG"
      },
      "source": [
        "# Correlation of the V23\n",
        "plotting(\"Class\", \"V23\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QzW6ENsGWhe"
      },
      "source": [
        "There is correlation Between V23 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers less than -40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjPABfZkDFdg"
      },
      "source": [
        "# Correlation of the V24\n",
        "plotting(\"Class\", \"V24\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXtnYkuFHxb3"
      },
      "source": [
        "There is correlation Between V24 and the Classification of the Fraud detection based on the above graphs and data dispersion even though looks like is slight\n",
        "\n",
        "Outliers greater than 2.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXCvbEf9DETb"
      },
      "source": [
        "# Correlation of the V25\n",
        "plotting(\"Class\", \"V25\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBR2Sl9bIbo_"
      },
      "source": [
        "There is some correlation Between V25 and the Classification of the Fraud detection based on the above graphs and data dispersion even though is quite slight\n",
        "\n",
        "Outliers less than -10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja9B2e21DDO3"
      },
      "source": [
        "# Correlation of the V26\n",
        "plotting(\"Class\", \"V26\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd4yvALUI4IX"
      },
      "source": [
        "There is correlation Between V26 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers less than -2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPW8X1B5DBhN"
      },
      "source": [
        "# Correlation of the V27\n",
        "plotting(\"Class\", \"V27\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJHMCYDpJeO0"
      },
      "source": [
        "There is small correlation Between V27 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater tha 13 and less than -10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keQViEy3DP32"
      },
      "source": [
        "# Correlation of the V28\n",
        "plotting(\"Class\", \"V28\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2LFu5Z7KClk"
      },
      "source": [
        "There is correlation Between V28 and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 20 less than -10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwex0xEzDTnS"
      },
      "source": [
        "# Correlation of the Amount\n",
        "plotting(\"Class\", \"Amount\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR4gszgFKUyd"
      },
      "source": [
        "There is correlation Between Amount and the Classification of the Fraud detection based on the above graphs and data dispersion\n",
        "\n",
        "Outliers greater than 6000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De5Ndz__nysV",
        "outputId": "7f3d90c2-e43f-4405-9ce3-50ca916a3348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "## Remove the outliers\n",
        "# first created the zip between the column and the value over which there are outliers\n",
        "vars_with_outliers_threshold_high = [(\"V4\", 15), (\"V5\", 40), (\"V6\", 25), (\"V7\", 60), (\"V9\", 15), (\"V10\", 20), (\"V13\", 6), (\"V15\", 6), (\"V16\", 10), (\"V20\", 27), (\"V21\", 20), (\"V22\", 10), (\"V24\", 25), (\"V27\", 13), (\"V28\", 20), (\"Amount\", 6000)]\n",
        "vars_with_outliers_threshold_low = [(\"V1\", -40), (\"V2\", -40), (\"V3\", -35), (\"V5\", -40), (\"V19\", -6), (\"V20\", -40), (\"V21\", -20), (\"V22\", -10), (\"V23\", -40), (\"V25\", -10), (\"V26\", -2), (\"V27\", -10), (\"V28\", -10)]\n",
        "\n",
        "# for loops to get rid of the outliers greater and lower than threshold\n",
        "for var_name, threshold in vars_with_outliers_threshold_high:\n",
        "    cc_train_df = cc_train_df[cc_train_df[var_name] < threshold]\n",
        "    \n",
        "cc_train_df\n",
        "\n",
        "for var_name, threshold in vars_with_outliers_threshold_low:\n",
        "    cc_train_df = cc_train_df[cc_train_df[var_name] > threshold]\n",
        "    \n",
        "cc_train_df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>33419.0</td>\n",
              "      <td>-2.178201</td>\n",
              "      <td>-3.132187</td>\n",
              "      <td>1.315758</td>\n",
              "      <td>-0.129783</td>\n",
              "      <td>-2.736013</td>\n",
              "      <td>0.743459</td>\n",
              "      <td>-0.752718</td>\n",
              "      <td>-2.650826</td>\n",
              "      <td>-0.184284</td>\n",
              "      <td>-1.392226</td>\n",
              "      <td>-1.114831</td>\n",
              "      <td>0.253591</td>\n",
              "      <td>-0.428280</td>\n",
              "      <td>-0.724290</td>\n",
              "      <td>-2.442338</td>\n",
              "      <td>0.649252</td>\n",
              "      <td>1.192440</td>\n",
              "      <td>-1.782696</td>\n",
              "      <td>0.014677</td>\n",
              "      <td>2.534626</td>\n",
              "      <td>-0.828762</td>\n",
              "      <td>-0.219136</td>\n",
              "      <td>-1.004913</td>\n",
              "      <td>0.788588</td>\n",
              "      <td>1.061994</td>\n",
              "      <td>-0.319407</td>\n",
              "      <td>-0.132313</td>\n",
              "      <td>0.333476</td>\n",
              "      <td>937.75</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>151317.0</td>\n",
              "      <td>2.064423</td>\n",
              "      <td>0.185575</td>\n",
              "      <td>-1.684612</td>\n",
              "      <td>0.411066</td>\n",
              "      <td>0.479555</td>\n",
              "      <td>-0.797963</td>\n",
              "      <td>0.205544</td>\n",
              "      <td>-0.240568</td>\n",
              "      <td>0.415454</td>\n",
              "      <td>-0.401418</td>\n",
              "      <td>-0.411296</td>\n",
              "      <td>0.439120</td>\n",
              "      <td>0.554022</td>\n",
              "      <td>-0.936324</td>\n",
              "      <td>0.196285</td>\n",
              "      <td>0.227611</td>\n",
              "      <td>0.510030</td>\n",
              "      <td>-0.465360</td>\n",
              "      <td>-0.031358</td>\n",
              "      <td>-0.130661</td>\n",
              "      <td>-0.351331</td>\n",
              "      <td>-0.876025</td>\n",
              "      <td>0.343288</td>\n",
              "      <td>0.522189</td>\n",
              "      <td>-0.259568</td>\n",
              "      <td>0.173623</td>\n",
              "      <td>-0.056280</td>\n",
              "      <td>-0.029665</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>132434.0</td>\n",
              "      <td>-0.547505</td>\n",
              "      <td>0.798072</td>\n",
              "      <td>-0.719939</td>\n",
              "      <td>-1.129561</td>\n",
              "      <td>0.925708</td>\n",
              "      <td>0.763338</td>\n",
              "      <td>0.231338</td>\n",
              "      <td>0.799204</td>\n",
              "      <td>-0.277812</td>\n",
              "      <td>-0.348452</td>\n",
              "      <td>0.511676</td>\n",
              "      <td>-0.034514</td>\n",
              "      <td>-1.390963</td>\n",
              "      <td>1.067757</td>\n",
              "      <td>0.423194</td>\n",
              "      <td>-0.262177</td>\n",
              "      <td>-0.003822</td>\n",
              "      <td>-0.601821</td>\n",
              "      <td>-0.988247</td>\n",
              "      <td>-0.313966</td>\n",
              "      <td>0.366664</td>\n",
              "      <td>1.068933</td>\n",
              "      <td>-0.101523</td>\n",
              "      <td>-1.604148</td>\n",
              "      <td>-0.318277</td>\n",
              "      <td>0.838076</td>\n",
              "      <td>0.012324</td>\n",
              "      <td>-0.015564</td>\n",
              "      <td>11.95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>81787.0</td>\n",
              "      <td>-0.945710</td>\n",
              "      <td>0.323579</td>\n",
              "      <td>0.595681</td>\n",
              "      <td>-1.288095</td>\n",
              "      <td>0.818906</td>\n",
              "      <td>-0.748491</td>\n",
              "      <td>0.890076</td>\n",
              "      <td>-0.130671</td>\n",
              "      <td>-0.471365</td>\n",
              "      <td>-0.389743</td>\n",
              "      <td>0.537702</td>\n",
              "      <td>0.137711</td>\n",
              "      <td>-0.555964</td>\n",
              "      <td>0.457870</td>\n",
              "      <td>-0.277456</td>\n",
              "      <td>0.662419</td>\n",
              "      <td>-0.951232</td>\n",
              "      <td>-0.103400</td>\n",
              "      <td>0.466225</td>\n",
              "      <td>-0.263767</td>\n",
              "      <td>-0.371528</td>\n",
              "      <td>-1.149510</td>\n",
              "      <td>0.217859</td>\n",
              "      <td>-0.507989</td>\n",
              "      <td>-0.026857</td>\n",
              "      <td>0.591496</td>\n",
              "      <td>-0.326179</td>\n",
              "      <td>-0.007543</td>\n",
              "      <td>24.98</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>125062.0</td>\n",
              "      <td>1.898722</td>\n",
              "      <td>-0.321038</td>\n",
              "      <td>-1.771837</td>\n",
              "      <td>0.672408</td>\n",
              "      <td>0.115019</td>\n",
              "      <td>-1.267347</td>\n",
              "      <td>0.612810</td>\n",
              "      <td>-0.441070</td>\n",
              "      <td>0.450298</td>\n",
              "      <td>0.107004</td>\n",
              "      <td>-1.144233</td>\n",
              "      <td>-0.035721</td>\n",
              "      <td>-0.728277</td>\n",
              "      <td>0.565399</td>\n",
              "      <td>-0.368894</td>\n",
              "      <td>-0.494280</td>\n",
              "      <td>-0.107078</td>\n",
              "      <td>-0.658594</td>\n",
              "      <td>0.169095</td>\n",
              "      <td>-0.082106</td>\n",
              "      <td>0.015111</td>\n",
              "      <td>0.006269</td>\n",
              "      <td>-0.029094</td>\n",
              "      <td>-0.071333</td>\n",
              "      <td>0.179444</td>\n",
              "      <td>0.378225</td>\n",
              "      <td>-0.106042</td>\n",
              "      <td>-0.059506</td>\n",
              "      <td>104.36</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199360</th>\n",
              "      <td>140177.0</td>\n",
              "      <td>2.164466</td>\n",
              "      <td>-1.721358</td>\n",
              "      <td>0.118546</td>\n",
              "      <td>-1.363516</td>\n",
              "      <td>-2.147961</td>\n",
              "      <td>-0.486731</td>\n",
              "      <td>-1.755439</td>\n",
              "      <td>-0.023992</td>\n",
              "      <td>-0.563216</td>\n",
              "      <td>1.457773</td>\n",
              "      <td>-1.097192</td>\n",
              "      <td>-0.654407</td>\n",
              "      <td>0.621714</td>\n",
              "      <td>-0.925904</td>\n",
              "      <td>0.076346</td>\n",
              "      <td>-0.105142</td>\n",
              "      <td>0.329165</td>\n",
              "      <td>0.269093</td>\n",
              "      <td>-0.346530</td>\n",
              "      <td>-0.356177</td>\n",
              "      <td>-0.123266</td>\n",
              "      <td>0.138772</td>\n",
              "      <td>0.319598</td>\n",
              "      <td>0.018634</td>\n",
              "      <td>-0.560967</td>\n",
              "      <td>-0.251774</td>\n",
              "      <td>0.060706</td>\n",
              "      <td>-0.023599</td>\n",
              "      <td>48.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199361</th>\n",
              "      <td>71541.0</td>\n",
              "      <td>1.070134</td>\n",
              "      <td>0.181228</td>\n",
              "      <td>0.670405</td>\n",
              "      <td>1.351215</td>\n",
              "      <td>-0.617646</td>\n",
              "      <td>-1.019820</td>\n",
              "      <td>0.203157</td>\n",
              "      <td>-0.194571</td>\n",
              "      <td>-0.043719</td>\n",
              "      <td>-0.006449</td>\n",
              "      <td>-0.021080</td>\n",
              "      <td>0.380856</td>\n",
              "      <td>-0.011871</td>\n",
              "      <td>0.369452</td>\n",
              "      <td>0.981649</td>\n",
              "      <td>0.211580</td>\n",
              "      <td>-0.374066</td>\n",
              "      <td>-0.524216</td>\n",
              "      <td>-0.440469</td>\n",
              "      <td>-0.018276</td>\n",
              "      <td>-0.260380</td>\n",
              "      <td>-0.931052</td>\n",
              "      <td>0.149692</td>\n",
              "      <td>0.647047</td>\n",
              "      <td>0.204529</td>\n",
              "      <td>-0.739666</td>\n",
              "      <td>0.012326</td>\n",
              "      <td>0.046917</td>\n",
              "      <td>70.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199362</th>\n",
              "      <td>128249.0</td>\n",
              "      <td>2.111207</td>\n",
              "      <td>-0.687175</td>\n",
              "      <td>-1.637544</td>\n",
              "      <td>-1.931276</td>\n",
              "      <td>-0.244434</td>\n",
              "      <td>-1.065946</td>\n",
              "      <td>-0.062955</td>\n",
              "      <td>-0.269111</td>\n",
              "      <td>2.439795</td>\n",
              "      <td>-1.334028</td>\n",
              "      <td>-0.857487</td>\n",
              "      <td>0.928522</td>\n",
              "      <td>-0.293575</td>\n",
              "      <td>0.177409</td>\n",
              "      <td>0.459425</td>\n",
              "      <td>-1.582800</td>\n",
              "      <td>0.227376</td>\n",
              "      <td>-0.173021</td>\n",
              "      <td>1.221354</td>\n",
              "      <td>-0.220178</td>\n",
              "      <td>0.115593</td>\n",
              "      <td>0.742015</td>\n",
              "      <td>-0.011124</td>\n",
              "      <td>0.731225</td>\n",
              "      <td>0.455436</td>\n",
              "      <td>-0.671152</td>\n",
              "      <td>0.039939</td>\n",
              "      <td>-0.051133</td>\n",
              "      <td>7.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199363</th>\n",
              "      <td>123408.0</td>\n",
              "      <td>1.893443</td>\n",
              "      <td>0.551151</td>\n",
              "      <td>-1.145498</td>\n",
              "      <td>3.889379</td>\n",
              "      <td>0.570851</td>\n",
              "      <td>-0.580469</td>\n",
              "      <td>0.635586</td>\n",
              "      <td>-0.266040</td>\n",
              "      <td>-1.032785</td>\n",
              "      <td>1.478749</td>\n",
              "      <td>-1.421178</td>\n",
              "      <td>-0.602395</td>\n",
              "      <td>-0.938759</td>\n",
              "      <td>0.562817</td>\n",
              "      <td>-1.291184</td>\n",
              "      <td>0.346647</td>\n",
              "      <td>-0.457061</td>\n",
              "      <td>-0.707557</td>\n",
              "      <td>-1.285782</td>\n",
              "      <td>-0.302385</td>\n",
              "      <td>-0.009592</td>\n",
              "      <td>-0.038580</td>\n",
              "      <td>0.078061</td>\n",
              "      <td>0.007246</td>\n",
              "      <td>0.183011</td>\n",
              "      <td>0.016792</td>\n",
              "      <td>-0.072975</td>\n",
              "      <td>-0.056060</td>\n",
              "      <td>37.92</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199364</th>\n",
              "      <td>160168.0</td>\n",
              "      <td>-6.067452</td>\n",
              "      <td>4.891426</td>\n",
              "      <td>-2.405573</td>\n",
              "      <td>-0.357747</td>\n",
              "      <td>-2.208001</td>\n",
              "      <td>2.946289</td>\n",
              "      <td>-7.743597</td>\n",
              "      <td>-14.272909</td>\n",
              "      <td>-0.509250</td>\n",
              "      <td>-2.603412</td>\n",
              "      <td>-1.928852</td>\n",
              "      <td>2.610036</td>\n",
              "      <td>0.184087</td>\n",
              "      <td>2.386579</td>\n",
              "      <td>-0.229910</td>\n",
              "      <td>1.089783</td>\n",
              "      <td>0.724788</td>\n",
              "      <td>0.664463</td>\n",
              "      <td>-0.931327</td>\n",
              "      <td>3.931755</td>\n",
              "      <td>-7.216839</td>\n",
              "      <td>3.488780</td>\n",
              "      <td>1.320159</td>\n",
              "      <td>0.598303</td>\n",
              "      <td>-0.308065</td>\n",
              "      <td>-0.261913</td>\n",
              "      <td>-0.260125</td>\n",
              "      <td>0.623409</td>\n",
              "      <td>15.25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>198736 rows Ã— 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Time        V1        V2  ...       V28  Amount  Class\n",
              "Id                                    ...                         \n",
              "1        33419.0 -2.178201 -3.132187  ...  0.333476  937.75      0\n",
              "2       151317.0  2.064423  0.185575  ... -0.029665    1.98      0\n",
              "3       132434.0 -0.547505  0.798072  ... -0.015564   11.95      0\n",
              "4        81787.0 -0.945710  0.323579  ... -0.007543   24.98      0\n",
              "5       125062.0  1.898722 -0.321038  ... -0.059506  104.36      0\n",
              "...          ...       ...       ...  ...       ...     ...    ...\n",
              "199360  140177.0  2.164466 -1.721358  ... -0.023599   48.00      0\n",
              "199361   71541.0  1.070134  0.181228  ...  0.046917   70.00      0\n",
              "199362  128249.0  2.111207 -0.687175  ... -0.051133    7.99      0\n",
              "199363  123408.0  1.893443  0.551151  ... -0.056060   37.92      0\n",
              "199364  160168.0 -6.067452  4.891426  ...  0.623409   15.25      0\n",
              "\n",
              "[198736 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7BGzQFTsnSF"
      },
      "source": [
        "Fron Training set 43 outliers have been removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL12JLPTtbIR"
      },
      "source": [
        "# correlation matrix between the features\n",
        "cc_train_df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJotqfyvFRFI",
        "outputId": "7d964bf3-377a-4b9c-ea8a-f7af55105f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "source": [
        "# correlation array between the Class output and the other features\n",
        "cc_train_df.corr()[\"Class\"]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Time     -0.012075\n",
              "V1       -0.095455\n",
              "V2        0.088403\n",
              "V3       -0.187832\n",
              "V4        0.132874\n",
              "V5       -0.089402\n",
              "V6       -0.048434\n",
              "V7       -0.178078\n",
              "V8        0.041927\n",
              "V9       -0.096322\n",
              "V10      -0.211447\n",
              "V11       0.155233\n",
              "V12      -0.257261\n",
              "V13      -0.003944\n",
              "V14      -0.307595\n",
              "V15      -0.003672\n",
              "V16      -0.191338\n",
              "V17      -0.319454\n",
              "V18      -0.105728\n",
              "V19       0.033639\n",
              "V20       0.022614\n",
              "V21       0.030573\n",
              "V22       0.003995\n",
              "V23      -0.012310\n",
              "V24      -0.006217\n",
              "V25       0.003205\n",
              "V26       0.004514\n",
              "V27       0.026787\n",
              "V28       0.010133\n",
              "Amount    0.007789\n",
              "Class     1.000000\n",
              "Name: Class, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKg7KfmlFL57"
      },
      "source": [
        "The Correlation matrix Shows that the correlation for V13, V15 is below -0.003 and V22, V25, V26 have correlation value smaller than 0.004\n",
        "So i think this 5 features can be discarded, this can be seen also in the heatmap for the training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNoCRhh-q-qa"
      },
      "source": [
        "# Heatmap of the data correlation of training set\n",
        "\n",
        "fig = px.imshow(cc_train_df.corr(), title=\"Correlation heatmap of the features from the training set\")\n",
        "fig.update_layout(height=1000)\n",
        "fig.update_xaxes(showticklabels=True, tickmode='linear').update_yaxes(showticklabels=True, tickmode='linear')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfci-QTvrVwr"
      },
      "source": [
        "# Heatmap of the data correlation of testing set\n",
        "\n",
        "fig = px.imshow(cc_test_df.corr(), title=\"Correlation heatmap of the features from the testing set\")\n",
        "fig.update_layout(height=800)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPQd0Net5Pp6"
      },
      "source": [
        "Found some features which have negligible correlation with the Class so will be removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S2dM6GYGw0O",
        "outputId": "01a263ae-0cbe-476d-a09b-a39f8e7e817e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "# to remove the features with negligible level of correlation\n",
        "\n",
        "columns_no_correlation = [\"V13\", \"V15\", \"V22\", \"V25\", \"V26\"]\n",
        "\n",
        "columns_with_correlation = [var for var in columns if var not in columns_no_correlation]\n",
        "columns_with_correlation"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Time',\n",
              " 'V1',\n",
              " 'V2',\n",
              " 'V3',\n",
              " 'V4',\n",
              " 'V5',\n",
              " 'V6',\n",
              " 'V7',\n",
              " 'V8',\n",
              " 'V9',\n",
              " 'V10',\n",
              " 'V11',\n",
              " 'V12',\n",
              " 'V14',\n",
              " 'V16',\n",
              " 'V17',\n",
              " 'V18',\n",
              " 'V19',\n",
              " 'V20',\n",
              " 'V21',\n",
              " 'V23',\n",
              " 'V24',\n",
              " 'V27',\n",
              " 'V28',\n",
              " 'Amount']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Mv24o3HqnW"
      },
      "source": [
        "## Statistical Modelling\n",
        "\n",
        "Statistical modelling of the data with Linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7iqJpOUHpyP"
      },
      "source": [
        "## Build a function to construct a formula string\n",
        "def build_formula_string(response_var, cat_vars, num_vars):\n",
        "    categorical_list = [\"C({})\".format(var) for var in cat_vars]\n",
        "    categorical_string = \" + \".join(categorical_list)\n",
        "    numeric_string = \" + \".join(num_vars)\n",
        "    formula_string = \"{} ~ {} + {}\".format(response_var, categorical_string, numeric_string)\n",
        "    \n",
        "    return formula_string\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIvVrL9nJM4G",
        "outputId": "5b5ae598-5333-4640-bba6-49db97c34116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# create the formula string to be used for statsmodel\n",
        "\n",
        "numeric_data = columns_with_correlation\n",
        "categorical_data = []\n",
        "\n",
        "formula_string = build_formula_string(\"Class\", categorical_data, numeric_data)\n",
        "\n",
        "formula_string"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Class ~  + Time + V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V14 + V16 + V17 + V18 + V19 + V20 + V21 + V23 + V24 + V27 + V28 + Amount'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxEkpl7_JsKx"
      },
      "source": [
        "# fitting the statsmodel and visualizing the summary\n",
        "\n",
        "baseline_model = smf.ols(formula_string, cc_train_df).fit()\n",
        "baseline_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQiUiCq-0kIj"
      },
      "source": [
        "## Function-ize the fitted vs residual plots, and the qq plot\n",
        "def plot_fitted_vs_resid(fitted_values, residuals, model_name):\n",
        "    fig = px.scatter(x=fitted_values, y=residuals, labels={\"x\": \"Fitted Values\", \"y\": \"Residuals\"}, \n",
        "                      title=\"{} model, fitted values vs residuals\".format(model_name))\n",
        "    fig.show()\n",
        "    \n",
        "def plot_qq(residuals, model_name):\n",
        "    fig = sm.qqplot(residuals, fit=True, line='45')\n",
        "    plt.title(\"QQ Plot of {} model residuals\".format(model_name))\n",
        "    plt.show()\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTxy9yMr1xLh"
      },
      "source": [
        "# To plot the residuals and the QQ plot\n",
        "plot_fitted_vs_resid(baseline_model.fittedvalues, baseline_model.resid, \"Baseline\")\n",
        "plot_qq(baseline_model.resid, \"Baseline\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFnqwN5q2cAR"
      },
      "source": [
        "Looking into the above graphs the  residual are not statistically independent \n",
        "and homoscedasicity, plus the residuals are not normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crGx42ln6_Mm"
      },
      "source": [
        "# function to augment the dataframe columns \n",
        "def augmented_dataframe(dataframe, response_col_name):\n",
        "  augmented_df = dataframe\n",
        "\n",
        "  ## Drop the response_variable from the augmented dataframe (we'll add it back in later)\n",
        "  augmented_df = augmented_df.drop(response_col_name, axis=1)\n",
        "\n",
        "  for col in augmented_df.columns:\n",
        "    augmented_df[col+\"_\"+str(2)] = augmented_df[col] ** 2\n",
        "    augmented_df[col+\"_\"+str(3)] = augmented_df[col] ** 3\n",
        "\n",
        "  ## Add response_col_name from dataframe back into augmented_df\n",
        "  augmented_df[response_col_name] = dataframe[response_col_name] \n",
        "\n",
        "  return augmented_df\n",
        "\n",
        "\n",
        "# function to prune the model with unnecessary features\n",
        "def prune_models(dataframe, response_col_name):\n",
        "\n",
        "  ## Define the first part of our formula string \n",
        "  init_formula_string = response_col_name + \" ~ \"\n",
        "    \n",
        "    ### Assign cols_no_response to the list of the dataframe columns\n",
        "  cols_no_response = dataframe.columns.tolist()\n",
        "    ### <Remove> the response column name\n",
        "  cols_no_response.remove(response_col_name)\n",
        "    ### <Join> the string (using ' + ') as the join seperator\n",
        "  rest_of_formula_string = \" + \".join(cols_no_response)\n",
        "    \n",
        "    ## Concat the two strings together (in a new variable) to obtain our full formula string \n",
        "  formula_string = init_formula_string + rest_of_formula_string\n",
        "\n",
        "    ## Fit the initial model variation \n",
        "  model_init = smf.ols(formula_string, data=dataframe).fit()\n",
        "    \n",
        "    ## Assign a p-values variable \n",
        "  pvalues = model_init.pvalues\n",
        "    \n",
        "    ## Find the variable with the highest p-value \n",
        "  max_p = pvalues.idxmax()\n",
        "    \n",
        "    ## alpha threshhold at 0.05 \n",
        "  alpha = 0.05\n",
        "    \n",
        "\n",
        "    # <While> loop over our p-values with the condition that the max value is above our alpha threshold \n",
        "    # Identify the variable with the maximum p-value \n",
        "    # Drop this variable from our cols_no_response list \n",
        "    # Create a new formula string \n",
        "    # Fit the new model \n",
        "    # Re-declare our p-values variable \n",
        "    # <Drop> the Intercept attribute \n",
        "    # Once our while loop breaks, return the model \n",
        "\n",
        "    \n",
        "  while pvalues.max() > alpha:\n",
        "\n",
        "    max_p = pvalues.idxmax()\n",
        "    cols_no_response.remove(max_p)\n",
        "    \n",
        "    rest_of_formula_string = \" + \".join(cols_no_response)\n",
        "    formula_string = init_formula_string + rest_of_formula_string\n",
        "        \n",
        "    model = smf.ols(formula_string, data=dataframe).fit()\n",
        "    pvalues = model.pvalues\n",
        "    pvalues = pvalues.drop(\"Intercept\")\n",
        "        \n",
        "  return model\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQa-2A3Z8sQn"
      },
      "source": [
        "# model with automatic pruning of the features \n",
        "\n",
        "model_with_pruned_df = prune_models(cc_train_df, \"Class\")\n",
        "model_with_pruned_df.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prFVdW-GCzUk"
      },
      "source": [
        "# augmentation and automatic pruning of the features\n",
        "augmented_test_df = augmented_dataframe(cc_train_df, \"Class\")\n",
        "\n",
        "model_aug_pruned_df = prune_models(augmented_test_df, \"Class\")\n",
        "model_aug_pruned_df.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbHsvkZ1D793"
      },
      "source": [
        "# To plot the residuals and the QQ plot\n",
        "plot_fitted_vs_resid(model_aug_pruned_df.fittedvalues, model_aug_pruned_df.resid, \"Baseline\")\n",
        "plot_qq(model_aug_pruned_df.resid, \"Baseline\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1M4uG9PE8HC"
      },
      "source": [
        "Tried the automatic pruning with alpha of 0.05 no improvement of the model adjusted r square\n",
        "\n",
        "Tried with augmentation up to cubic of all the columns of the dataset with automatic pruning\n",
        "the result of the  adjusted r squareis worst and theere is no improvements on the  residuals statistical independence and homoscedasicity, plus still the residuals are not normal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWquwPwXGH9l"
      },
      "source": [
        "# function to calculate the box cox transformation\n",
        "\n",
        "def calculate_and_plot_boxcox(variable, alpha=0.05, title_string=\"\", plot_lower_bound=-20, plot_upper_bound=20):\n",
        "    transformed_data, best_lambda, conf = stats.boxcox(variable, alpha=alpha)\n",
        "    lambdas, corr_coef = stats.boxcox_normplot(variable, plot_lower_bound, plot_upper_bound)\n",
        "    \n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=lambdas, y=corr_coef))\n",
        "    fig.add_shape(dict(type=\"line\", x0=best_lambda, x1=best_lambda, y0=0, y1=1))\n",
        "    fig.add_shape(dict(type=\"line\", x0=conf[0], x1=conf[0], y0=0, y1=1, line=dict(color=\"Red\", width=1)))\n",
        "    fig.add_shape(dict(type=\"line\", x0=conf[1], x1=conf[1], y0=0, y1=1, line=dict(color=\"Red\", width=1)))\n",
        "    fig.add_shape(dict(type=\"line\", x0=1, x1=1, y0=0, y1=1, line=dict(color=\"Red\", width=1, dash=\"dash\")))\n",
        "    fig.update_layout(xaxis_title=\"Lambda Value\", yaxis_title=\"Correlation Coefficient\", title=\"Box-Cox Normality Plot ({})\".format(title_string))\n",
        "    \n",
        "    # return the confidence interval too\n",
        "    return transformed_data, best_lambda, conf, fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeVEGM7RCS1J"
      },
      "source": [
        "As in the data frame there are columns with negative value the box cox transformations are not working\n",
        "\n",
        "So to over come the issue i'm going to implement the solution as per:\n",
        "https://stats.stackexchange.com/questions/399435/which-constant-to-add-when-applying-box-cox-transformation-to-negative-values\n",
        "whihc says to add a constant value to all the column of that partivular feature to make them all positive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH8Y5U2IASlS"
      },
      "source": [
        "# Function to make all the column with positive value\n",
        "# takes input a the training and test data frame and a list of columns\n",
        "# for loop ove the clumns and finds the minimum value between test and training set \n",
        "# and offsets both set for the minimum value per column if the minimum is negative\n",
        "\n",
        "def positive_df(training_df, test_df, columns_list):\n",
        "  for col in columns_list:\n",
        "    min  = training_df[col].min()\n",
        "    if min > test_df[col].min():\n",
        "      min = test_df[col].min()\n",
        "    \n",
        "    if min < 0:\n",
        "      training_df[col] = training_df[col] - min\n",
        "      test_df[col] = test_df[col] - min\n",
        "  \n",
        "  return training_df, test_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvS1wFnLI5jP"
      },
      "source": [
        "# make positive all the dataframe\n",
        "\n",
        "ps_train_df, ps_test_df = positive_df(cc_train_df, cc_test_df, columns_with_correlation)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhVmSR96GmiG"
      },
      "source": [
        "## Loop over the numeric data and print the variable name alongside the best lambda transformation and it's confidence interval\n",
        "## going to use the columns_with_correlation previously calculated\n",
        "\n",
        "eps = 1e-6\n",
        "numeric_data = columns_with_correlation\n",
        "for var in numeric_data:\n",
        "    _, best_lambda, conf, _ = calculate_and_plot_boxcox(ps_train_df[var] + eps)\n",
        "    print(var, \"\\t\", best_lambda, \"\\t\", conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCXIK7TXdO4x"
      },
      "source": [
        "I'm going to ignore all the transformation greater than 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cpzIIGbV9qa"
      },
      "source": [
        "ps_train_df[\"Time_trans\"] = ps_train_df[\"Time\"]**0.5  # the lambda found is 0.77 so approximated to 0.5\n",
        "ps_test_df[\"Time_trans\"] = ps_test_df[\"Time\"]**0.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V4_trans\"] = ps_train_df[\"V4\"]**0.5  # the lambda found is 0.56 so approximated to 0.5\n",
        "ps_test_df[\"V4_trans\"] = ps_test_df[\"V4\"]**0.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V5_trans\"] = ps_train_df[\"V5\"]**1.5  # the lambda found is 1.5 \n",
        "ps_test_df[\"V5_trans\"] = ps_test_df[\"V5\"]**1.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V6_trans\"] = ps_train_df[\"V6\"]**0.5  # the lambda found is 0.41 so approximated to 0.5\n",
        "ps_test_df[\"V6_trans\"] = ps_test_df[\"V6\"]**0.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V7_trans\"] = ps_train_df[\"V7\"]**1.5  # the lambda found is 1.54 so approximated to 1.5\n",
        "ps_test_df[\"V7_trans\"] = ps_test_df[\"V7\"]**1.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V9_trans\"] = ps_train_df[\"V9\"]**0.5  # the lambda found is 0.47 so approximated to 0.5\n",
        "ps_test_df[\"V9_trans\"] = ps_test_df[\"V9\"]**0.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V10_trans\"] = ps_train_df[\"V10\"]**0.5  # the lambda found is 0.72 so approximated to 0.5\n",
        "ps_test_df[\"V10_trans\"] = ps_test_df[\"V10\"]**0.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V11_trans\"] = ps_train_df[\"V11\"]**0.5  # the lambda found is 0.58 so approximated to 0.5\n",
        "ps_test_df[\"V11_trans\"] = ps_test_df[\"V11\"]**0.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V14_trans\"] = ps_train_df[\"V14\"]**3  # the lambda found is 3.46 so approximated to 3\n",
        "ps_test_df[\"V14_trans\"] = ps_test_df[\"V14\"]**3    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V16_trans\"] = ps_train_df[\"V16\"]**3  # the lambda found is 3.12 so approximated to 3\n",
        "ps_test_df[\"V16_trans\"] = ps_test_df[\"V16\"]**3    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V17_trans\"] = ps_train_df[\"V17\"]**3  # the lambda found is 2.8 so approximated to 3\n",
        "ps_test_df[\"V17_trans\"] = ps_test_df[\"V17\"]**3    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V18_trans\"] = ps_train_df[\"V18\"]**1.5  # the lambda found is 1.56 so approximated to 1.5\n",
        "ps_test_df[\"V18_trans\"] = ps_test_df[\"V18\"]**1.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V20_trans\"] = ps_train_df[\"V20\"]**2  # the lambda found is 1.96 so approximated to 2\n",
        "ps_test_df[\"V20_trans\"] = ps_test_df[\"V20\"]**2    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V21_trans\"] = np.log(ps_train_df[\"V21\"] + eps)  # the lambda found is 0.15 so approximated to log\n",
        "ps_test_df[\"V21_trans\"] = np.log(ps_test_df[\"V21\"] + eps)    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V23_trans\"] = ps_train_df[\"V23\"]**2  # the lambda found is 2.08 so approximated to 2\n",
        "ps_test_df[\"V23_trans\"] = ps_test_df[\"V23\"]**2    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V24_trans\"] = ps_train_df[\"V24\"]**1.5  # the lambda found is 1.7 so approximated to 1.5\n",
        "ps_test_df[\"V24_trans\"] = ps_test_df[\"V24\"]**1.5    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"V27_trans\"] = ps_train_df[\"V27\"]**2  # the lambda found is 1.8 so approximated to 2\n",
        "ps_test_df[\"V27_trans\"] = ps_test_df[\"V27\"]**2    # same transformation on the testing set\n",
        "\n",
        "ps_train_df[\"Amount_trans\"] = np.log(ps_train_df[\"Amount\"] + eps)  # the lambda found is 0.12 so approximated to log\n",
        "ps_test_df[\"Amount_trans\"] = np.log(ps_test_df[\"Amount\"] + eps)    # same transformation on the testing set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYvYc655eP53"
      },
      "source": [
        "# Column with the new names\n",
        "column_cor_transform = ['Time_trans', 'V1', 'V2', 'V3', 'V4_trans', 'V5_trans', 'V6_trans', 'V7_trans', 'V8', 'V9_trans', 'V10_trans', 'V11_trans', 'V12', 'V14_trans', \n",
        "                        'V16_trans', 'V17_trans', 'V18_trans', 'V19', 'V20_trans', 'V21_trans', 'V23_trans', 'V24_trans', 'V27_trans', 'V28', 'Amount_trans']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcbHt_RKhPGP"
      },
      "source": [
        "# create the statistical model with the new columns with the transformation\n",
        "numeric_data = column_cor_transform\n",
        "categorical_data = []\n",
        "\n",
        "formula_string_trans = build_formula_string(\"Class\", categorical_data, numeric_data)\n",
        "\n",
        "transformed_df_model = smf.ols(formula_string_trans, ps_train_df).fit()\n",
        "transformed_df_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsqBJKV9jH3S"
      },
      "source": [
        "# To plot the residuals and the QQ plot\n",
        "plot_fitted_vs_resid(transformed_df_model.fittedvalues, transformed_df_model.resid, \"Baseline\")\n",
        "plot_qq(transformed_df_model.resid, \"Baseline\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4waZNWT47sd8"
      },
      "source": [
        "All the Above was done under the assumption that the use of linear regression model needs to be used along with his assumption but that is my wrong assumption\n",
        "\n",
        "The Logistic regression needs to be used along with his assumption which are different from the Linear regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upAvxM5BYkPz"
      },
      "source": [
        "**Logistic regression with stats model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnMSsxSAAPmF"
      },
      "source": [
        "Use of the stats model to check the influential Features and to drop the feature which are not relevant to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vviedxl5Ye56",
        "outputId": "b1c22810-58b1-4987-8e23-8d6640b3ccca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "source": [
        "base_regression_model = smf.logit(formula_string, cc_train_df).fit()\n",
        "\n",
        "base_regression_model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003979\n",
            "         Iterations 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>198736</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>198710</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    25</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Sun, 25 Oct 2020</td> <th>  Pseudo R-squ.:     </th>  <td>0.6938</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>22:16:23</td>     <th>  Log-Likelihood:    </th> <td> -790.73</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -2582.0</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>   -8.5744</td> <td>    0.279</td> <td>  -30.757</td> <td> 0.000</td> <td>   -9.121</td> <td>   -8.028</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time</th>      <td>-2.694e-07</td> <td> 2.41e-06</td> <td>   -0.112</td> <td> 0.911</td> <td>   -5e-06</td> <td> 4.46e-06</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V1</th>        <td>    0.1193</td> <td>    0.050</td> <td>    2.380</td> <td> 0.017</td> <td>    0.021</td> <td>    0.218</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V2</th>        <td>    0.0155</td> <td>    0.064</td> <td>    0.245</td> <td> 0.807</td> <td>   -0.109</td> <td>    0.140</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V3</th>        <td>    0.0465</td> <td>    0.062</td> <td>    0.747</td> <td> 0.455</td> <td>   -0.075</td> <td>    0.168</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V4</th>        <td>    0.6662</td> <td>    0.083</td> <td>    8.038</td> <td> 0.000</td> <td>    0.504</td> <td>    0.829</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V5</th>        <td>    0.1340</td> <td>    0.073</td> <td>    1.836</td> <td> 0.066</td> <td>   -0.009</td> <td>    0.277</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V6</th>        <td>   -0.1045</td> <td>    0.080</td> <td>   -1.312</td> <td> 0.190</td> <td>   -0.261</td> <td>    0.052</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V7</th>        <td>   -0.1018</td> <td>    0.077</td> <td>   -1.327</td> <td> 0.185</td> <td>   -0.252</td> <td>    0.049</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V8</th>        <td>   -0.2185</td> <td>    0.039</td> <td>   -5.653</td> <td> 0.000</td> <td>   -0.294</td> <td>   -0.143</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V9</th>        <td>   -0.2826</td> <td>    0.122</td> <td>   -2.323</td> <td> 0.020</td> <td>   -0.521</td> <td>   -0.044</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V10</th>       <td>   -0.7896</td> <td>    0.115</td> <td>   -6.869</td> <td> 0.000</td> <td>   -1.015</td> <td>   -0.564</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V11</th>       <td>    0.0283</td> <td>    0.096</td> <td>    0.295</td> <td> 0.768</td> <td>   -0.160</td> <td>    0.216</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V12</th>       <td>    0.0433</td> <td>    0.098</td> <td>    0.442</td> <td> 0.659</td> <td>   -0.149</td> <td>    0.235</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V14</th>       <td>   -0.5573</td> <td>    0.074</td> <td>   -7.522</td> <td> 0.000</td> <td>   -0.703</td> <td>   -0.412</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V16</th>       <td>   -0.2578</td> <td>    0.129</td> <td>   -1.993</td> <td> 0.046</td> <td>   -0.511</td> <td>   -0.004</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V17</th>       <td>   -0.1284</td> <td>    0.081</td> <td>   -1.583</td> <td> 0.113</td> <td>   -0.287</td> <td>    0.031</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V18</th>       <td>    0.0535</td> <td>    0.139</td> <td>    0.385</td> <td> 0.701</td> <td>   -0.219</td> <td>    0.326</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V19</th>       <td>    0.0544</td> <td>    0.106</td> <td>    0.515</td> <td> 0.607</td> <td>   -0.153</td> <td>    0.262</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V20</th>       <td>   -0.4094</td> <td>    0.106</td> <td>   -3.864</td> <td> 0.000</td> <td>   -0.617</td> <td>   -0.202</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V21</th>       <td>    0.1701</td> <td>    0.054</td> <td>    3.123</td> <td> 0.002</td> <td>    0.063</td> <td>    0.277</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V23</th>       <td>   -0.0580</td> <td>    0.061</td> <td>   -0.947</td> <td> 0.344</td> <td>   -0.178</td> <td>    0.062</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V24</th>       <td>    0.2765</td> <td>    0.171</td> <td>    1.621</td> <td> 0.105</td> <td>   -0.058</td> <td>    0.611</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V27</th>       <td>   -0.9471</td> <td>    0.164</td> <td>   -5.775</td> <td> 0.000</td> <td>   -1.269</td> <td>   -0.626</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V28</th>       <td>   -0.4333</td> <td>    0.149</td> <td>   -2.909</td> <td> 0.004</td> <td>   -0.725</td> <td>   -0.141</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Amount</th>    <td>    0.0013</td> <td>    0.000</td> <td>    2.797</td> <td> 0.005</td> <td>    0.000</td> <td>    0.002</td>\n",
              "</tr>\n",
              "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.24 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:                  Class   No. Observations:               198736\n",
              "Model:                          Logit   Df Residuals:                   198710\n",
              "Method:                           MLE   Df Model:                           25\n",
              "Date:                Sun, 25 Oct 2020   Pseudo R-squ.:                  0.6938\n",
              "Time:                        22:16:23   Log-Likelihood:                -790.73\n",
              "converged:                       True   LL-Null:                       -2582.0\n",
              "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
              "==============================================================================\n",
              "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept     -8.5744      0.279    -30.757      0.000      -9.121      -8.028\n",
              "Time       -2.694e-07   2.41e-06     -0.112      0.911      -5e-06    4.46e-06\n",
              "V1             0.1193      0.050      2.380      0.017       0.021       0.218\n",
              "V2             0.0155      0.064      0.245      0.807      -0.109       0.140\n",
              "V3             0.0465      0.062      0.747      0.455      -0.075       0.168\n",
              "V4             0.6662      0.083      8.038      0.000       0.504       0.829\n",
              "V5             0.1340      0.073      1.836      0.066      -0.009       0.277\n",
              "V6            -0.1045      0.080     -1.312      0.190      -0.261       0.052\n",
              "V7            -0.1018      0.077     -1.327      0.185      -0.252       0.049\n",
              "V8            -0.2185      0.039     -5.653      0.000      -0.294      -0.143\n",
              "V9            -0.2826      0.122     -2.323      0.020      -0.521      -0.044\n",
              "V10           -0.7896      0.115     -6.869      0.000      -1.015      -0.564\n",
              "V11            0.0283      0.096      0.295      0.768      -0.160       0.216\n",
              "V12            0.0433      0.098      0.442      0.659      -0.149       0.235\n",
              "V14           -0.5573      0.074     -7.522      0.000      -0.703      -0.412\n",
              "V16           -0.2578      0.129     -1.993      0.046      -0.511      -0.004\n",
              "V17           -0.1284      0.081     -1.583      0.113      -0.287       0.031\n",
              "V18            0.0535      0.139      0.385      0.701      -0.219       0.326\n",
              "V19            0.0544      0.106      0.515      0.607      -0.153       0.262\n",
              "V20           -0.4094      0.106     -3.864      0.000      -0.617      -0.202\n",
              "V21            0.1701      0.054      3.123      0.002       0.063       0.277\n",
              "V23           -0.0580      0.061     -0.947      0.344      -0.178       0.062\n",
              "V24            0.2765      0.171      1.621      0.105      -0.058       0.611\n",
              "V27           -0.9471      0.164     -5.775      0.000      -1.269      -0.626\n",
              "V28           -0.4333      0.149     -2.909      0.004      -0.725      -0.141\n",
              "Amount         0.0013      0.000      2.797      0.005       0.000       0.002\n",
              "==============================================================================\n",
              "\n",
              "Possibly complete quasi-separation: A fraction 0.24 of observations can be\n",
              "perfectly predicted. This might indicate that there is complete\n",
              "quasi-separation. In this case some parameters will not be identified.\n",
              "\"\"\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg7yXcHgyujA"
      },
      "source": [
        "The logistic regression has different assumption and with logistic regression, the focus is on assessing the model's adequacy \n",
        "\n",
        "\n",
        "\n",
        "Reference: https://www.pythonfordatascience.org/logistic-regression-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnv8ArryzUUa",
        "outputId": "aea66541-9de9-4a8b-bb21-5e91ee6682bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        }
      },
      "source": [
        "## Plotting multiple plots same figure\n",
        "fig, (axL, axR) = plt.subplots(2, figsize=(15, 15))\n",
        "plt.suptitle(\"Logistic Regression Residual Plots \\n using Seaborn Lowess line (N = 400)\")\n",
        "\n",
        "\n",
        "# Deviance Residuals\n",
        "sns.regplot(x=base_regression_model.fittedvalues, y=base_regression_model.resid_dev, ax= axL,\n",
        "            color=\"black\", scatter_kws={\"s\": 5},\n",
        "            line_kws={\"color\":\"b\", \"alpha\":1, \"lw\":2}, lowess=True)\n",
        "\n",
        "axL.set_title(\"Deviance Residuals \\n against Fitted Values\")\n",
        "axL.set_xlabel(\"Linear Predictor Values\")\n",
        "axL.set_ylabel(\"Deviance Residuals\")\n",
        "\n",
        "# Studentized Pearson Residuals\n",
        "sns.regplot(x=base_regression_model.fittedvalues, y=base_regression_model.resid_pearson, ax= axR,\n",
        "            color=\"black\", scatter_kws={\"s\": 5},\n",
        "            line_kws={\"color\":\"g\", \"alpha\":1, \"lw\":2}, lowess=True)\n",
        "\n",
        "axR.set_title(\"Studentized Pearson Residuals \\n against Fitted Values\")\n",
        "axR.set_xlabel(\"Linear Predictor Values\")\n",
        "axR.set_ylabel(\"Studentized Pearson Residuals\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_fjp-Lca5iM"
      },
      "source": [
        "From : https://medium.com/@dhiraj8899/top-5-assumptions-for-logistic-regression-96b11d24d357\n",
        "\n",
        "There are 5 assumption for the logistic regression:\n",
        "\n",
        "1.   The logistic regression assumes that there is minimal or no multicollinearity among the independent variables. \n",
        "2.   The Logistic regression assumes that the independent variables are linearly related to the log of odds.\n",
        "3. The logistic regression usually requires a large sample size to predict properly.\n",
        "4. The Logistic regression which has two classes assumes that the dependent variable is binary and ordered logistic regression requires the dependent variable to be ordered, for example Too Little, About Right, Too Much.\n",
        "5. The Logistic regression assumes the observations to be independent of each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDTz890bbZYU"
      },
      "source": [
        "Check on the Logistic regression assumption\n",
        "\n",
        "1.   The heatmap of the dataframe shows no correlation between the features\n",
        "2.   The graphs of logit value against the features should be linear \n",
        "3.   The datset is quite large so should satisfy the assumption\n",
        "4.   The logistic regression has one class with binary value so should satify the assumption\n",
        "5.   The data don't have the duplicates so the observation are indipendent\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfRk3tpZguUW",
        "outputId": "76c96ad8-5bdc-4112-8f19-d34cb73173f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "# Anova doesn't run on the logistic regression logit model\n",
        "\n",
        "sm.stats.anova_lm(base_regression_model, test=\"Chisq\", typ=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-dfcd7c83d54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Anova doesn't run on the logistic regression model logit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manova_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_regression_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Chisq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/statsmodels/stats/anova.py\u001b[0m in \u001b[0;36manova_lm\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0manova_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/statsmodels/stats/anova.py\u001b[0m in \u001b[0;36manova_single\u001b[0;34m(model, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"III\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         return anova3_lm_single(model, design_info, n_rows, test, pr_test,\n\u001b[0;32m---> 86\u001b[0;31m                                 robust)\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IV\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type IV not yet implemented\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/statsmodels/stats/anova.py\u001b[0m in \u001b[0;36manova3_lm_single\u001b[0;34m(model, design_info, n_rows, test, pr_test, robust)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m#table = table.iloc[np.argsort(col_order + [model.model.exog.shape[1]+1])]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# back out sum of squares from f_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mssr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'df'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_resid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sum_sq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# fill in residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/statsmodels/base/wrapper.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_attrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LogitResults' object has no attribute 'ssr'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw6Gafe3A3BO"
      },
      "source": [
        "As the Anova doesn't run other way needs to be defined to check the influential and non influential features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo07MQ5Lqb1I",
        "outputId": "ce496c98-7146-4c92-804e-e7ff94680285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        }
      },
      "source": [
        "base_regression_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>198736</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>198710</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    25</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Sat, 24 Oct 2020</td> <th>  Pseudo R-squ.:     </th>  <td>0.6938</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>17:31:04</td>     <th>  Log-Likelihood:    </th> <td> -790.73</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -2582.0</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>   -8.5744</td> <td>    0.279</td> <td>  -30.757</td> <td> 0.000</td> <td>   -9.121</td> <td>   -8.028</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time</th>      <td>-2.694e-07</td> <td> 2.41e-06</td> <td>   -0.112</td> <td> 0.911</td> <td>   -5e-06</td> <td> 4.46e-06</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V1</th>        <td>    0.1193</td> <td>    0.050</td> <td>    2.380</td> <td> 0.017</td> <td>    0.021</td> <td>    0.218</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V2</th>        <td>    0.0155</td> <td>    0.064</td> <td>    0.245</td> <td> 0.807</td> <td>   -0.109</td> <td>    0.140</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V3</th>        <td>    0.0465</td> <td>    0.062</td> <td>    0.747</td> <td> 0.455</td> <td>   -0.075</td> <td>    0.168</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V4</th>        <td>    0.6662</td> <td>    0.083</td> <td>    8.038</td> <td> 0.000</td> <td>    0.504</td> <td>    0.829</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V5</th>        <td>    0.1340</td> <td>    0.073</td> <td>    1.836</td> <td> 0.066</td> <td>   -0.009</td> <td>    0.277</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V6</th>        <td>   -0.1045</td> <td>    0.080</td> <td>   -1.312</td> <td> 0.190</td> <td>   -0.261</td> <td>    0.052</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V7</th>        <td>   -0.1018</td> <td>    0.077</td> <td>   -1.327</td> <td> 0.185</td> <td>   -0.252</td> <td>    0.049</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V8</th>        <td>   -0.2185</td> <td>    0.039</td> <td>   -5.653</td> <td> 0.000</td> <td>   -0.294</td> <td>   -0.143</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V9</th>        <td>   -0.2826</td> <td>    0.122</td> <td>   -2.323</td> <td> 0.020</td> <td>   -0.521</td> <td>   -0.044</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V10</th>       <td>   -0.7896</td> <td>    0.115</td> <td>   -6.869</td> <td> 0.000</td> <td>   -1.015</td> <td>   -0.564</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V11</th>       <td>    0.0283</td> <td>    0.096</td> <td>    0.295</td> <td> 0.768</td> <td>   -0.160</td> <td>    0.216</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V12</th>       <td>    0.0433</td> <td>    0.098</td> <td>    0.442</td> <td> 0.659</td> <td>   -0.149</td> <td>    0.235</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V14</th>       <td>   -0.5573</td> <td>    0.074</td> <td>   -7.522</td> <td> 0.000</td> <td>   -0.703</td> <td>   -0.412</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V16</th>       <td>   -0.2578</td> <td>    0.129</td> <td>   -1.993</td> <td> 0.046</td> <td>   -0.511</td> <td>   -0.004</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V17</th>       <td>   -0.1284</td> <td>    0.081</td> <td>   -1.583</td> <td> 0.113</td> <td>   -0.287</td> <td>    0.031</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V18</th>       <td>    0.0535</td> <td>    0.139</td> <td>    0.385</td> <td> 0.701</td> <td>   -0.219</td> <td>    0.326</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V19</th>       <td>    0.0544</td> <td>    0.106</td> <td>    0.515</td> <td> 0.607</td> <td>   -0.153</td> <td>    0.262</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V20</th>       <td>   -0.4094</td> <td>    0.106</td> <td>   -3.864</td> <td> 0.000</td> <td>   -0.617</td> <td>   -0.202</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V21</th>       <td>    0.1701</td> <td>    0.054</td> <td>    3.123</td> <td> 0.002</td> <td>    0.063</td> <td>    0.277</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V23</th>       <td>   -0.0580</td> <td>    0.061</td> <td>   -0.947</td> <td> 0.344</td> <td>   -0.178</td> <td>    0.062</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V24</th>       <td>    0.2765</td> <td>    0.171</td> <td>    1.621</td> <td> 0.105</td> <td>   -0.058</td> <td>    0.611</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V27</th>       <td>   -0.9471</td> <td>    0.164</td> <td>   -5.775</td> <td> 0.000</td> <td>   -1.269</td> <td>   -0.626</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V28</th>       <td>   -0.4333</td> <td>    0.149</td> <td>   -2.909</td> <td> 0.004</td> <td>   -0.725</td> <td>   -0.141</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Amount</th>    <td>    0.0013</td> <td>    0.000</td> <td>    2.797</td> <td> 0.005</td> <td>    0.000</td> <td>    0.002</td>\n",
              "</tr>\n",
              "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.24 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:                  Class   No. Observations:               198736\n",
              "Model:                          Logit   Df Residuals:                   198710\n",
              "Method:                           MLE   Df Model:                           25\n",
              "Date:                Sat, 24 Oct 2020   Pseudo R-squ.:                  0.6938\n",
              "Time:                        17:31:04   Log-Likelihood:                -790.73\n",
              "converged:                       True   LL-Null:                       -2582.0\n",
              "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
              "==============================================================================\n",
              "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept     -8.5744      0.279    -30.757      0.000      -9.121      -8.028\n",
              "Time       -2.694e-07   2.41e-06     -0.112      0.911      -5e-06    4.46e-06\n",
              "V1             0.1193      0.050      2.380      0.017       0.021       0.218\n",
              "V2             0.0155      0.064      0.245      0.807      -0.109       0.140\n",
              "V3             0.0465      0.062      0.747      0.455      -0.075       0.168\n",
              "V4             0.6662      0.083      8.038      0.000       0.504       0.829\n",
              "V5             0.1340      0.073      1.836      0.066      -0.009       0.277\n",
              "V6            -0.1045      0.080     -1.312      0.190      -0.261       0.052\n",
              "V7            -0.1018      0.077     -1.327      0.185      -0.252       0.049\n",
              "V8            -0.2185      0.039     -5.653      0.000      -0.294      -0.143\n",
              "V9            -0.2826      0.122     -2.323      0.020      -0.521      -0.044\n",
              "V10           -0.7896      0.115     -6.869      0.000      -1.015      -0.564\n",
              "V11            0.0283      0.096      0.295      0.768      -0.160       0.216\n",
              "V12            0.0433      0.098      0.442      0.659      -0.149       0.235\n",
              "V14           -0.5573      0.074     -7.522      0.000      -0.703      -0.412\n",
              "V16           -0.2578      0.129     -1.993      0.046      -0.511      -0.004\n",
              "V17           -0.1284      0.081     -1.583      0.113      -0.287       0.031\n",
              "V18            0.0535      0.139      0.385      0.701      -0.219       0.326\n",
              "V19            0.0544      0.106      0.515      0.607      -0.153       0.262\n",
              "V20           -0.4094      0.106     -3.864      0.000      -0.617      -0.202\n",
              "V21            0.1701      0.054      3.123      0.002       0.063       0.277\n",
              "V23           -0.0580      0.061     -0.947      0.344      -0.178       0.062\n",
              "V24            0.2765      0.171      1.621      0.105      -0.058       0.611\n",
              "V27           -0.9471      0.164     -5.775      0.000      -1.269      -0.626\n",
              "V28           -0.4333      0.149     -2.909      0.004      -0.725      -0.141\n",
              "Amount         0.0013      0.000      2.797      0.005       0.000       0.002\n",
              "==============================================================================\n",
              "\n",
              "Possibly complete quasi-separation: A fraction 0.24 of observations can be\n",
              "perfectly predicted. This might indicate that there is complete\n",
              "quasi-separation. In this case some parameters will not be identified.\n",
              "\"\"\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RvocWsEsB2_"
      },
      "source": [
        "From Above summary of the logistic model shows that the P values for some of the features are too big and their coefficient is too small, so they are not influential and can be pruned\n",
        "\n",
        "So will run a prune model for the logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knRgvF3jrBnI"
      },
      "source": [
        "# function to prune the model with unnecessary features\n",
        "def prune_models_logit(dataframe, response_col_name):\n",
        "\n",
        "  ## Define the first part of our formula string \n",
        "  init_formula_string = response_col_name + \" ~ \"\n",
        "    \n",
        "    ### Assign cols_no_response to the list of the dataframe columns\n",
        "  cols_no_response = dataframe.columns.tolist()\n",
        "    ### <Remove> the response column name\n",
        "  cols_no_response.remove(response_col_name)\n",
        "    ### <Join> the string (using ' + ') as the join seperator\n",
        "  rest_of_formula_string = \" + \".join(cols_no_response)\n",
        "    \n",
        "    ## Concat the two strings together (in a new variable) to obtain our full formula string \n",
        "  formula_string = init_formula_string + rest_of_formula_string\n",
        "\n",
        "    ## Fit the initial model variation \n",
        "  model_init = smf.ols(formula_string, data=dataframe).fit()\n",
        "    \n",
        "    ## Assign a p-values variable \n",
        "  pvalues = model_init.pvalues\n",
        "    \n",
        "    ## Find the variable with the highest p-value \n",
        "  max_p = pvalues.idxmax()\n",
        "    \n",
        "    ## alpha threshhold at 0.05 \n",
        "  alpha = 0.05\n",
        "    \n",
        "\n",
        "    # <While> loop over our p-values with the condition that the max value is above our alpha threshold \n",
        "    # Identify the variable with the maximum p-value \n",
        "    # Drop this variable from our cols_no_response list \n",
        "    # Create a new formula string \n",
        "    # Fit the new model \n",
        "    # Re-declare our p-values variable \n",
        "    # <Drop> the Intercept attribute \n",
        "    # Once our while loop breaks, return the model \n",
        "\n",
        "    \n",
        "  while pvalues.max() > alpha:\n",
        "\n",
        "    max_p = pvalues.idxmax()\n",
        "    cols_no_response.remove(max_p)\n",
        "    \n",
        "    rest_of_formula_string = \" + \".join(cols_no_response)\n",
        "    formula_string = init_formula_string + rest_of_formula_string\n",
        "        \n",
        "    model = smf.logit(formula_string, data=dataframe).fit()\n",
        "    pvalues = model.pvalues\n",
        "    pvalues = pvalues.drop(\"Intercept\")\n",
        "        \n",
        "  return model, cols_no_response"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3rbk-ZxrTch",
        "outputId": "4f22d2bc-560f-49c1-90e2-65a9c74e8685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_with_pruned_df, columns_with_correlation_LR = prune_models_logit(cc_train_df, \"Class\")\n",
        "print(columns_with_correlation_LR)\n",
        "model_with_pruned_df.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003913\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003913\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003913\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003913\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003914\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003914\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003916\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003917\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003922\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003927\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003931\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003936\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003943\n",
            "         Iterations 13\n",
            "['V1', 'V4', 'V5', 'V7', 'V8', 'V9', 'V10', 'V13', 'V14', 'V16', 'V20', 'V21', 'V22', 'V24', 'V27', 'V28', 'Amount']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>Logit Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>198736</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>198718</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    17</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>            <td>Sun, 25 Oct 2020</td> <th>  Pseudo R-squ.:     </th>  <td>0.6965</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                <td>22:17:18</td>     <th>  Log-Likelihood:    </th> <td> -783.53</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -2582.0</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Intercept</th> <td>   -8.7244</td> <td>    0.158</td> <td>  -55.228</td> <td> 0.000</td> <td>   -9.034</td> <td>   -8.415</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V1</th>        <td>    0.1115</td> <td>    0.045</td> <td>    2.488</td> <td> 0.013</td> <td>    0.024</td> <td>    0.199</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V4</th>        <td>    0.6223</td> <td>    0.072</td> <td>    8.664</td> <td> 0.000</td> <td>    0.482</td> <td>    0.763</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V5</th>        <td>    0.1577</td> <td>    0.040</td> <td>    3.990</td> <td> 0.000</td> <td>    0.080</td> <td>    0.235</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V7</th>        <td>   -0.1808</td> <td>    0.067</td> <td>   -2.705</td> <td> 0.007</td> <td>   -0.312</td> <td>   -0.050</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V8</th>        <td>   -0.1669</td> <td>    0.030</td> <td>   -5.626</td> <td> 0.000</td> <td>   -0.225</td> <td>   -0.109</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V9</th>        <td>   -0.2862</td> <td>    0.101</td> <td>   -2.841</td> <td> 0.004</td> <td>   -0.484</td> <td>   -0.089</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V10</th>       <td>   -0.7212</td> <td>    0.103</td> <td>   -7.020</td> <td> 0.000</td> <td>   -0.923</td> <td>   -0.520</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V13</th>       <td>   -0.2932</td> <td>    0.095</td> <td>   -3.077</td> <td> 0.002</td> <td>   -0.480</td> <td>   -0.106</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V14</th>       <td>   -0.6106</td> <td>    0.064</td> <td>   -9.522</td> <td> 0.000</td> <td>   -0.736</td> <td>   -0.485</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V16</th>       <td>   -0.3023</td> <td>    0.073</td> <td>   -4.161</td> <td> 0.000</td> <td>   -0.445</td> <td>   -0.160</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V20</th>       <td>   -0.4528</td> <td>    0.089</td> <td>   -5.070</td> <td> 0.000</td> <td>   -0.628</td> <td>   -0.278</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V21</th>       <td>    0.3080</td> <td>    0.064</td> <td>    4.778</td> <td> 0.000</td> <td>    0.182</td> <td>    0.434</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V22</th>       <td>    0.5206</td> <td>    0.144</td> <td>    3.624</td> <td> 0.000</td> <td>    0.239</td> <td>    0.802</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V24</th>       <td>    0.3892</td> <td>    0.163</td> <td>    2.395</td> <td> 0.017</td> <td>    0.071</td> <td>    0.708</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V27</th>       <td>   -0.8868</td> <td>    0.144</td> <td>   -6.146</td> <td> 0.000</td> <td>   -1.170</td> <td>   -0.604</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>V28</th>       <td>   -0.4709</td> <td>    0.157</td> <td>   -2.995</td> <td> 0.003</td> <td>   -0.779</td> <td>   -0.163</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Amount</th>    <td>    0.0016</td> <td>    0.000</td> <td>    4.104</td> <td> 0.000</td> <td>    0.001</td> <td>    0.002</td>\n",
              "</tr>\n",
              "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.29 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
            ],
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                           Logit Regression Results                           \n",
              "==============================================================================\n",
              "Dep. Variable:                  Class   No. Observations:               198736\n",
              "Model:                          Logit   Df Residuals:                   198718\n",
              "Method:                           MLE   Df Model:                           17\n",
              "Date:                Sun, 25 Oct 2020   Pseudo R-squ.:                  0.6965\n",
              "Time:                        22:17:18   Log-Likelihood:                -783.53\n",
              "converged:                       True   LL-Null:                       -2582.0\n",
              "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
              "==============================================================================\n",
              "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
              "------------------------------------------------------------------------------\n",
              "Intercept     -8.7244      0.158    -55.228      0.000      -9.034      -8.415\n",
              "V1             0.1115      0.045      2.488      0.013       0.024       0.199\n",
              "V4             0.6223      0.072      8.664      0.000       0.482       0.763\n",
              "V5             0.1577      0.040      3.990      0.000       0.080       0.235\n",
              "V7            -0.1808      0.067     -2.705      0.007      -0.312      -0.050\n",
              "V8            -0.1669      0.030     -5.626      0.000      -0.225      -0.109\n",
              "V9            -0.2862      0.101     -2.841      0.004      -0.484      -0.089\n",
              "V10           -0.7212      0.103     -7.020      0.000      -0.923      -0.520\n",
              "V13           -0.2932      0.095     -3.077      0.002      -0.480      -0.106\n",
              "V14           -0.6106      0.064     -9.522      0.000      -0.736      -0.485\n",
              "V16           -0.3023      0.073     -4.161      0.000      -0.445      -0.160\n",
              "V20           -0.4528      0.089     -5.070      0.000      -0.628      -0.278\n",
              "V21            0.3080      0.064      4.778      0.000       0.182       0.434\n",
              "V22            0.5206      0.144      3.624      0.000       0.239       0.802\n",
              "V24            0.3892      0.163      2.395      0.017       0.071       0.708\n",
              "V27           -0.8868      0.144     -6.146      0.000      -1.170      -0.604\n",
              "V28           -0.4709      0.157     -2.995      0.003      -0.779      -0.163\n",
              "Amount         0.0016      0.000      4.104      0.000       0.001       0.002\n",
              "==============================================================================\n",
              "\n",
              "Possibly complete quasi-separation: A fraction 0.29 of observations can be\n",
              "perfectly predicted. This might indicate that there is complete\n",
              "quasi-separation. In this case some parameters will not be identified.\n",
              "\"\"\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpEQrVCisVdq"
      },
      "source": [
        "The pruning of the feature has improved slightly the pseudo r square and now all the feature columns have the P value smaller than 0.05 as required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr23w8imz4Yo"
      },
      "source": [
        "# Final dataset with only the required comlumns\n",
        "\n",
        "columns_final_dataset = columns_with_correlation_LR + [\"Class\"]\n",
        "\n",
        "train_df_final = cc_train_df [columns_final_dataset]\n",
        "test_df_final = cc_test_df [columns_final_dataset]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0ltXbah1Sw3"
      },
      "source": [
        "# dataset saved with pickle in the google drive\n",
        "\n",
        "pickle.dump(train_df_final, open(\"/content/drive/My Drive/Colab Notebooks/train_df_final.pkl\", \"wb\"))\n",
        "pickle.dump(test_df_final, open(\"/content/drive/My Drive/Colab Notebooks/test_df_final.pkl\", \"wb\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzbMl6OJvMAk"
      },
      "source": [
        "### Top Most Influential variables\n",
        "\n",
        "To check the influential variables:\n",
        "1. First will calculate the R squared with all the above found columns as base model\n",
        "2. After for each of the column i will take them out one by one from the model and calculate the R squared\n",
        "3. The differenz between the base model and the each model without the particular feature will be their contribution to the R squared"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwG_isySwIHe"
      },
      "source": [
        "# define the function as described above \n",
        "\n",
        "def most_influential_var(dataframe, response_col_name, features):\n",
        "\n",
        "  categorical_data = []\n",
        "\n",
        "  formula_string_trans = build_formula_string(response_col_name, categorical_data, features)\n",
        "  model_init = smf.logit(formula_string_trans, data=dataframe).fit()\n",
        "  rsquared_model = model_init.prsquared\n",
        "\n",
        "  rsquared =  []\n",
        "\n",
        "  for i, col in enumerate(features):\n",
        "\n",
        "    columns = [var for var in features if var not in col]\n",
        "    formula_string_trans = build_formula_string(response_col_name, categorical_data, columns)\n",
        "    model = smf.logit(formula_string_trans, data=dataframe).fit()\n",
        "    rsquared_dif= rsquared_model - model.prsquared\n",
        "    rsquared.append([col, rsquared_dif])\n",
        "      \n",
        "  return rsquared\n",
        "  "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh-QGmaUz_4m",
        "outputId": "af005f6a-a237-4923-ae22-fdeccadd6113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        }
      },
      "source": [
        "# run the function and the output is an array of feature and their respective R squared contribution\n",
        "influential_features = most_influential_var(train_df_final, \"Class\", columns_with_correlation_LR)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003943\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003959\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.004162\n",
            "         Iterations 12\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003975\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003959\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003995\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003963\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.004087\n",
            "         Iterations 12\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003987\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.004223\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.004011\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003998\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003994\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003977\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003958\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.004013\n",
            "         Iterations 12\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003972\n",
            "         Iterations 13\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.003976\n",
            "         Iterations 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymytOHnh4_eB",
        "outputId": "e1f340bc-2b7a-4fa9-ad8d-8f17fb786bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "# Converted the array to the dataframe to easily sort and show the fitrst 8 with the highest contribution\n",
        "df = pd.DataFrame(influential_features, columns=['Features', 'R Squared Contribution',])\n",
        "df.sort_values(by='R Squared Contribution', ascending=False).head(8)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Features</th>\n",
              "      <th>R Squared Contribution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>V14</td>\n",
              "      <td>0.021618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>V4</td>\n",
              "      <td>0.016923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>V10</td>\n",
              "      <td>0.011148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>V27</td>\n",
              "      <td>0.005416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>V16</td>\n",
              "      <td>0.005305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>V20</td>\n",
              "      <td>0.004246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>V8</td>\n",
              "      <td>0.004013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>V21</td>\n",
              "      <td>0.003956</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Features  R Squared Contribution\n",
              "8       V14                0.021618\n",
              "1        V4                0.016923\n",
              "6       V10                0.011148\n",
              "14      V27                0.005416\n",
              "9       V16                0.005305\n",
              "10      V20                0.004246\n",
              "4        V8                0.004013\n",
              "11      V21                0.003956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}